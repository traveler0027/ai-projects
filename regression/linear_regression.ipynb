{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">Linear Regression</h2>\n",
    "<p>The idea of regression is to predict continous values</P>\n",
    "<p>The goal of Linear Regression is to approximate the best fit linear equation to a plot as shown:</p>\n",
    "<img src=\"../images/linear_regression.png\">\n",
    "<p>From Algebra, we recall that the linear line equation is:</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = wx + b\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>where:</p>\n",
    "<p style=\"text-align:center;\"><i>y_hat</i>  is the approximation</p>\n",
    "<p style=\"text-align:center;\"><i>w</i>  is the slop or weight</p>\n",
    "<p style=\"text-align:center;\"><i>b</i>  is the intercept or bias</p>\n",
    "<p style=\"text-align:center;\"><i>x</i>  is the input variable</p>\n",
    "\n",
    "<p>to figure out 'w' and 'b' we will need to recursively update the values</p>\n",
    "<p> this can be done by the <strong>Mean Squared Error</strong> (the cost function)</p>\n",
    "<p>we want to find the difference between the actual value <i>y_i</i> and approximated value <i> (wx +b)</i> for all elelements <i>n</i>.</p>\n",
    "<p>the value is squared to avoid negatives and sum is divided by the number of samples to give us the mean</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "MSE = J(w,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (w x_i + b))^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mean Squared Error</h2><p>(cost function)</p>\n",
    "<p>will need to be minimized</p>\n",
    "<p> to do this, we can take the derivitave of each varaible with respect to the other</p>\n",
    "<p>this is also called the gradient</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "J'(m,b) =\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{df}{dw}\\\\\n",
    "     \\frac{df}{db}\\\\\n",
    "    \\end{bmatrix} =\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{1}{N} \\sum -2x_i(y_i - (wx_i + b)) \\\\\n",
    "     \\frac{1}{N} \\sum -2(y_i - (wx_i + b)) \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The gradiant works to find the minimum cost function</p>\n",
    "<img src=\"../images/gradient.png\">\n",
    "<p>with each iteration, the weights and bias are updated and moved towards the global minimum</p>\n",
    "<p>the rules for updating the weights and bais are:</p>\n",
    "<p>where alpha is the learning rate and <i>dw</i> or <i>db</i> is the derivative</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "w = w - \\alpha \\cdot dw\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "b = b - \\alpha \\cdot db\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>the derivitives can be found as follows</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "dw = \\frac{dJ}{dw} = \\frac{1}{N} \\sum_{i=1}^{n} -2x_i(y_i - (wx_i + b)) =\n",
    "\\frac{1}{N} \\sum_{i=1}^{n} -2x_i(y_i - \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{n} 2x_i(\\hat{y} - y_i)\n",
    "\\end{align}\n",
    "\\begin{align}\\newline\\end{align}\n",
    "\\begin{align}\n",
    "db = \\frac{dJ}{db} = \\frac{1}{N} \\sum_{i=1}^{n} -2(y_i - (wx_i + b)) =\n",
    "\\frac{1}{N} \\sum_{i=1}^{n} -2(y_i - \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{n} 2(\\hat{y} - y_i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>the learning rate determines how 'fast' the varaibles approach the global minimum. A large learning rate can approach it faster but has a risk of over shooting. While a small learning rate would approach it smaller but has less risk.</p>\n",
    "<img src=\"../images/learning_rate.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Algorithm Code</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__ (self,lr=0.001,n_iters=1000):\n",
    "        #lr is the learning rate. Needs to be a small number\n",
    "        #n_iters is number of iterations\n",
    "        \n",
    "        #store the learning rate and number of iterations\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        #weights and bias will be defined later, but nned to be declared here\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        #X is training samples\n",
    "        #y is training labels\n",
    "        #initiate parameters\n",
    "        n_samples,n_features = X.shape\n",
    "        \n",
    "        #initialize weights and bais as an array of 0's of size n\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        #implement gradiant decent\n",
    "        for _ in range(self.n_iters):\n",
    "            y_predicted = np.dot(X,self.weights) + self.bias\n",
    "            \n",
    "            #calculate the derivatives\n",
    "            dw = (1/n_samples)*np.dot(X.T,(y_predicted - y))\n",
    "            db = (1/n_samples)*np.sum(y_predicted - y)\n",
    "            \n",
    "            self.weights -= self.lr*dw\n",
    "            self.bias -= self.lr*db\n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "        #predict labels for X\n",
    "        y_predicted = np.dot(X,self.weights) + self.bias\n",
    "        return y_predicted\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>lets see it in action</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  380.1306497140645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X,y = datasets.make_regression(n_samples=100,n_features=1,noise=20, random_state=4)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=123)\n",
    "\n",
    "regressor = LinearRegression(lr=0.01)\n",
    "#we want to fit the train samples and training labels\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "#predict the labels to the test samples\n",
    "predicted = regressor.predict(X_test)\n",
    "\n",
    "#check performance\n",
    "def mse(y_true,y_predicted):\n",
    "    return np.mean((y_true-y_predicted)**2)\n",
    "\n",
    "mse_value = mse(y_test, predicted)\n",
    "print('Mean Squared Error: ',mse_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
